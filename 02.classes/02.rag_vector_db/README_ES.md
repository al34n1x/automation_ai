# üß† Generaci√≥n Aumentada por Recuperaci√≥n (RAG) y Bases de Datos Vectoriales ‚Äî Para Automatizaci√≥n con IA (con n8n)

> **Audiencia**: Desde principiantes hasta nivel intermedio
> **Objetivo**: Entender RAG de punta a punta e implementar un flujo RAG funcional en n8n usando una base de datos vectorial.
> **Qu√© obtendr√°s**: Explicaciones en lenguaje sencillo, diagramas ASCII, laboratorios paso a paso, prompts listos para copiar y pegar, ejemplos de cargas API y resoluci√≥n de problemas.

## üìö Temario (Le√© en orden)
1) **Visi√≥n general**: Qu√© problema resuelve RAG (en palabras simples)
2) **Conceptos**: Embeddings, vectores, similitud, fragmentaci√≥n (chunking)
3) **Arquitectura**: C√≥mo fluye un sistema RAG (diagramas ASCII)
4) **Herramientas**: Opciones de bases de datos vectoriales y cu√°ndo usarlas
5) **Implementaci√≥n en n8n**: Paso a paso
6) **Ejemplos para copiar y pegar**: Prompts, cuerpos, curl
7) **Pruebas y evaluaci√≥n**: ¬øFunciona?
8) **Resoluci√≥n de problemas y FAQs**
9) **Seguridad, costos y buenas pr√°cticas**
10) **Glosario**


## 1) Visi√≥n general ‚Äî ¬øQu√© problema resuelve RAG?
Los Modelos de Lenguaje Grande (LLM) son excelentes para escribir, explicar y resumir, pero pueden adivinar cuando no saben algo o cuando las pol√≠ticas, precios o documentaci√≥n de tu empresa no estaban en sus datos de entrenamiento. Esto puede producir respuestas muy seguras pero equivocadas.

RAG = Retrieval-Augmented Generation (Generaci√≥n aumentada por recuperaci√≥n)
En lugar de pedirle a la IA que ‚Äúrecuerde todo‚Äù, recuperamos los pasajes correctos de tus datos (documentos, wikis, hojas de c√°lculo, correos, PDFs) en el momento de la pregunta y se los damos al modelo para fundamentar su respuesta.

Pensalo como un examen con libro abierto:

- **Sin RAG:** El alumno responde solo de memoria.
- **Con RAG:** El alumno busca r√°pidamente la p√°gina relevante en el libro y luego responde.

**Resultados**: respuestas m√°s precisas, actualizadas y explicables‚Äîsin reentrenar el modelo.


## 2) Conceptos b√°sicos (en lenguaje claro)

### 2.1 Embeddings (convertir texto en n√∫meros)
Transformamos texto como ‚Äúpol√≠tica de reembolso‚Äù en una lista de n√∫meros llamada vector. Este vector captura el significado del texto. Significados similares ‚Üí vectores similares.

```
"pol√≠tica de reembolso"  ‚Üí  [0.12, -0.45, 0.88, ...]  
"pol√≠tica de devoluciones" ‚Üí [0.11, -0.43, 0.90, ...]   (muy cercano en significado)
```

### 2.2 Bases de datos vectoriales
Una base de datos vectorial almacena muchos de estos vectores (provenientes de tus documentos). Cuando un usuario hace una pregunta:

1) **Creamos un embedding de la pregunta.**
2) Buscamos en la base de datos los vectores m√°s similares (top-k resultados).
3) Le damos esos pasajes de texto a la IA para que responda con precisi√≥n.

Ejemplos populares: Pinecone, Weaviate, Milvus, Qdrant.

### 2.3 B√∫squeda por similitud
Medimos la cercan√≠a usando similitud coseno o distancia euclidiana. No necesit√°s la matem√°tica: ‚Äúmayor similitud = pasaje m√°s relevante‚Äù.

### 2.4 Fragmentaci√≥n (chunking)
Dividimos tus documentos en peque√±os fragmentos antes de hacer embeddings. Tama√±os t√≠picos: 500‚Äì1.000 tokens (aprox. palabras), con un peque√±o solapamiento para no cortar ideas a la mitad. Un buen chunking es el factor #1 para la calidad en RAG.

3) Arquitectura ‚Äî C√≥mo funciona RAG (paso a paso)

### 3.1 Flujo de alto nivel

```
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ   Pregunta del   ‚îÇ  ej: ‚Äú¬øCu√°l es nuestra pol√≠tica de reembolso?‚Äù
       ‚îÇ      usuario     ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Embedding de la ‚îÇ  (texto ‚Üí vector)
       ‚îÇ     consulta     ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ B√∫squeda en base ‚îÇ  (top-k fragmentos similares)
       ‚îÇ   de vectores    ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Construir prompt‚îÇ  (pregunta + fragmentos recuperados)
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Respuesta del LLM‚îÇ  (basada en tus datos)
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

```

### 3.2 Lo que necesit√°s
- **Modelo de embeddings** (ej: OpenAI embeddings u otro proveedor)
- **Base de datos vectorial** (hosteada o local)
- **LLM** (OpenAI/Anthropic/etc., o modelo local)
- **Orquestador** (usaremos n8n para integrarlo todo)

4) Elegir una base de datos vectorial (gu√≠a simple)

| Option   | Simple to Start | Scales Well | Notes |
|---------|------------------|-------------|-------|
| Pinecone| ‚≠ê‚≠ê‚≠ê‚≠ê            | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | Fully‚Äëmanaged, easy APIs |
| Weaviate| ‚≠ê‚≠ê‚≠ê             | ‚≠ê‚≠ê‚≠ê‚≠ê       | Open‚Äësource & hosted options |
| Milvus  | ‚≠ê‚≠ê              | ‚≠ê‚≠ê‚≠ê‚≠ê       | Popular, performance‚Äëfocused |
| Qdrant  | ‚≠ê‚≠ê‚≠ê             | ‚≠ê‚≠ê‚≠ê‚≠ê       | Open‚Äësource & hosted options |

Tip: Si quer√©s algo que ‚Äúsimplemente funcione‚Äù, empez√° con Pinecone (cloud). Si quer√©s open-source, Weaviate o Qdrant son excelentes.

## 5) Implementar RAG en n8n (pasos listos para pegar)

(Aqu√≠ seguir√≠an las secciones con el detalle de nodos, ejemplos curl, preparaci√≥n de datos, m√©tricas de calidad, troubleshooting, seguridad, buenas pr√°cticas, FAQ y glosario, todo traducido manteniendo formato y c√≥digo igual al original pero en espa√±ol.)

### 5.1 Workflow Overview (n8n)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User Trigger ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ Embedding API Call‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ Vector DB Search‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                      ‚îÇ
                                                      ‚ñº
                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                            ‚îÇ  Build RAG Prompt  ‚îÇ
                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                      ‚îÇ
                                                      ‚ñº
                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                            ‚îÇ     LLM Answer     ‚îÇ
                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                      ‚îÇ
                                                      ‚ñº
                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                            ‚îÇ Send Back to User  ‚îÇ
                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.2 Nodos (Paso a paso)

#### (A) Disparador (Trigger)
- Usar **Telegram Trigger**, **Slack Trigger** o **Webhook**.  
- La salida debe contener la **pregunta** del usuario en un campo como `$json.text` o `$json.query`.

#### (B) Create Embedding (OpenAI example via HTTP Request)
- **Method:** `POST`  
- **URL:** `https://api.openai.com/v1/embeddings`  
- **Headers:**  
  - `Authorization: Bearer {{ $env.OPENAI_API_KEY }}`  
  - `Content-Type: application/json`
- **Body (JSON):**
```json
{
  "input": "{{ $json.query || $json.text }}",
  "model": "text-embedding-3-small"
}
```
- **Tip de mapeo:** Crear un nuevo campo `embedding` con `{{$json.data[0].embedding}}` usando un nodo **Set** despu√©s de la llamada.

#### (C) Query Vector DB (Pinecone example)
- **Method:** `POST`  
- **URL:** `https://YOUR_INDEX_NAME-YOUR_PROJECT.svc.YOUR_REGION.pinecone.io/query`  
- **Headers:**  
  - `Api-Key: {{ $env.PINECONE_API_KEY }}`  
  - `Content-Type: application/json`
- **Body (JSON):**
```json
{
  "vector": {{ $json.embedding }},
  "topK": 4,
  "includeMetadata": true
}
```
- **Mapeo de resultados:** Esperar algo como `matches[].metadata.text` (puede variar seg√∫n tu formato de carga).

#### (D) Build the RAG Prompt (Set Node)
- **Prop√≥sito:** Combinar la pregunta del usuario + pasajes recuperados.  
- **Plantilla:**
```
Eres un asistente √∫til para nuestra organizaci√≥n.
Responde la pregunta usando SOLO el contexto de abajo. Si la respuesta no est√° en el contexto, di ‚ÄúNo tengo esa informaci√≥n‚Äù.

[Context]
{{ $json.matches.map(m => m.metadata.text).join("\n\n---\n\n") }}

[Question]
{{ $json.query || $json.text }}
```
- Guardar este texto completo en un campo llamado `rag_prompt`.

#### (E) Ask the LLM (OpenAI Chat Completions via HTTP)
- **Method:** `POST`  
- **URL:** `https://api.openai.com/v1/chat/completions`  
- **Headers:**  
  - `Authorization: Bearer {{ $env.OPENAI_API_KEY }}`  
  - `Content-Type: application/json`
- **Body (JSON):**
```json
{
  "model": "gpt-4o-mini",
  "messages": [
    { "role": "system", "content": "You only answer from the provided context. If unsure, say you don‚Äôt know." },
    { "role": "user", "content": "{{ $json.rag_prompt }}" }
  ],
  "temperature": 0.2
}
```
- **Seleccionar salida:** `choices[0].message.content` como `final_answer`.

#### (F) Return to User
- Usar **Telegram**, **Slack** o **Webhook Response** para enviar `$json.final_answer`.


## 6) Copy‚ÄëPaste: Example `curl` & Payloads

> √ötiles para pruebas r√°pidas fuera de n8n (por ejemplo, en tu terminal).


### 6.2 Consulta a Pinecone (reemplaza la URL por la de tu √≠ndice)
```bash
curl https://api.openai.com/v1/embeddings \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": "What is our refund policy?",
    "model": "text-embedding-3-small"
  }'
```

### 6.2 Pinecone Query (replace URL with your index endpoint)
```bash
curl https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_REGION.pinecone.io/query \
  -H "Api-Key: $PINECONE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "vector": [/* your vector numbers here */],
    "topK": 4,
    "includeMetadata": true
  }'
```

### 6.3 Chat Completion (OpenAI)
```bash
curl https://api.openai.com/v1/chat/completions \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "system", "content": "Answer only from the provided context."},
      {"role": "user", "content": "CONTEXT: ... \n\nQUESTION: What is our refund policy?"}
    ],
    "temperature": 0.2
  }'
```



## 7) Preparar tus datos (para que la b√∫squeda funcione)

**1) Recolectar fuentes:** PDFs, p√°ginas web, Google Docs/Sheets, Confluence, correos electr√≥nicos.  
**2) Limpiar y normalizar:** Quitar encabezados/pies de p√°gina, corregir saltos de l√≠nea extra√±os, convertir a texto plano.  
**3) Fragmentar:** 500‚Äì1.000 tokens con **solapamiento (50‚Äì200)** para que las ideas no se corten.  
**4) Metadatos:** Guardar etiquetas √∫tiles: `title`, `source`, `date`, `url`, `category`, `confidentiality`.  
**5) Insertar (Upsert):** Generar embeddings de cada fragmento y guardarlos en tu base vectorial con metadatos.

**Por qu√© importa el chunking:**  
- Muy chico ‚áí no hay suficiente contexto.  
- Muy grande ‚áí m√°s ruido y b√∫squeda m√°s lenta.  
- Punto √≥ptimo ‚áí probar; empezar en ~800 tokens + 100 de solapamiento.



## 8) Calidad: C√≥mo saber si RAG ‚Äúfunciona‚Äù

- **Precisi√≥n@k:** Al traer los top-k pasajes, ¬øson realmente relevantes?  
- **Fidelidad de la respuesta:** ¬øLa IA responde solo usando el contexto dado?  
- **Citas con referencia:** Opcional, incluir qu√© IDs de fragmento o URLs se usaron.  
- **Chequeo humano:** Crear 10‚Äì20 preguntas comunes y verificar respuestas.

**Tip:** Bajar `temperature` (0.0‚Äì0.3) para respuestas m√°s deterministas y factuales.



## 9) Resoluci√≥n de problemas

- **Respuestas malas o gen√©ricas:**  
  - Revisar chunking y metadatos.  
  - Aumentar `topK` (ej. de 3 ‚Üí 6).  
  - Hacer el prompt del sistema m√°s estricto (‚ÄúSi no est√° en el contexto, di que no lo sabes‚Äù).

- **Sin resultados de la base vectorial:**  
  - Confirmar que realmente insertaste los vectores.  
  - Verificar nombre de √≠ndice y entorno.  
  - Comprobar que est√°s enviando el **embedding de la consulta** (no el texto crudo).

- **Alucinaciones (informaci√≥n inventada):**  
  - Agregar: ‚ÄúSi no est√° en el contexto o no est√°s seguro, di que no lo sabes.‚Äù  
  - Reducir la temperatura.  
  - Usar ventanas de contexto m√°s peque√±as con pasajes m√°s espec√≠ficos.

- **Costos altos:**  
  - Usar modelos de embeddings m√°s baratos.  
  - Cachear embeddings (no recalcular los mismos documentos).  
  - Filtrar por metadatos antes de buscar (ej. solo documentos de ‚ÄúPol√≠tica de reembolso‚Äù).



## 10) Seguridad y gobernanza

- **Control de acceso:** Filtrar por rol de usuario o departamento (metadatos + pre-filtros).  
- **PII/Secretos:** No indexar datos sensibles; redactar antes de insertarlos.  
- **Residencia de datos:** Elegir regi√≥n que cumpla normativa.  
- **Auditabilidad:** Registrar qu√© fragmentos se recuperaron para cada respuesta.



## 11) Buenas pr√°cticas (r√°pidas de aplicar)

- Buen chunking + solapamiento  
- Texto limpio (sin n√∫meros de p√°gina o plantillas repetitivas)  
- A√±adir metadatos (t√≠tulo, secci√≥n, URL)  
- Indicar al modelo que **solo** use el contexto dado  
- Mantener temperatura baja para tareas factuales  
- Empezar peque√±o, medir y luego escalar



## 12) Preguntas frecuentes (FAQ)

**P: ¬øNecesito afinar (fine-tune) el modelo?**  
R: Generalmente no. RAG evita costos de entrenamiento recuperando la informaci√≥n al momento de la respuesta.

**P: ¬øQu√© tan grandes pueden ser mis documentos?**  
R: De cualquier tama√±o, mientras los **fragmentes** antes de generar embeddings.

**P: ¬øY si mis datos son privados?**  
R: Alojar tu propia base vectorial o usar un proveedor con seguridad y control regional s√≥lidos. Nunca indexar secretos.

**P: ¬øPuedo usar hojas de c√°lculo como fuente?**  
R: S√≠‚Äîexportar a CSV o conectarse con una API, luego fragmentar filas o secciones como texto.

**P: ¬øQu√© es ‚ÄútopK‚Äù?**  
R: La cantidad de fragmentos a recuperar. Probar entre 3‚Äì8 y evaluar la calidad.



## 13) Valores recomendados (copiar tal cual)

- Tama√±o de chunk: **800 tokens**, solapamiento **100**  
- topK: **4**  
- Temperatura: **0.2**  
- Guardrails de prompt: ‚ÄúResponder solo con el contexto; si no lo sab√©s, decir que no lo sab√©s.‚Äù



## 14) Glosario (muy breve)

- **Embedding:** Convertir texto en un vector (n√∫meros) que captura su significado.  
- **Vector:** Lista de n√∫meros que representan el significado del texto.  
- **Base vectorial:** Base optimizada para buscar por similitud de vectores.  
- **Chunking:** Partir documentos en fragmentos antes de generar embeddings.  
- **topK:** N√∫mero de fragmentos m√°s similares a recuperar.  
- **RAG:** Retrieval-Augmented Generation‚Äîrespuestas fundamentadas en texto recuperado.



## 15) Cr√©ditos y pr√≥ximos pasos

- Explorar la documentaci√≥n de Pinecone, Weaviate, Milvus o Qdrant.  
- Probar distintos modelos de embeddings y tama√±os de fragmentos.  
- A√±adir filtrado por rol para respetar el acceso a datos.  
- Medir calidad con un set reducido de preguntas reales.

¬°Feliz construcci√≥n! üöÄ