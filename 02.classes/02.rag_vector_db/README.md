# üß† Retrieval‚ÄëAugmented Generation (RAG) & Vector Search Databases ‚Äî For AI Automation (with n8n)

> **Audience:** Non‚Äëtechnical to intermediate learners  
> **Goal:** Understand RAG end‚Äëto‚Äëend and implement a working RAG workflow in **n8n** using a vector database.  
> **What you‚Äôll get:** Plain‚Äëlanguage explanations, ASCII diagrams, step‚Äëby‚Äëstep labs, copy‚Äëpaste prompts, example API payloads, and troubleshooting.

---

## üìö Syllabus (Read Me in Order)

1) Big Picture: What RAG solves (in simple words)  
2) Concepts: Embeddings, vectors, similarity, chunking  
3) Architecture: How a RAG system flows (ASCII diagrams)  
4) Tooling: Vector DB options & when to use them  
5) Implementation in **n8n** (step‚Äëby‚Äëstep)  
6) Copy‚Äëpaste examples (prompts, bodies, curl)  
7) Testing & evaluation (does it work?)  
8) Troubleshooting & FAQs  
9) Security, cost, and best practices  
10) Glossary

---

## 1) Big Picture ‚Äî What Problem Does RAG Solve?

Large Language Models (LLMs) are great at writing, explaining, and summarizing‚Äîbut they can **guess** when they don‚Äôt know something or when your company‚Äôs policies, prices, or docs weren‚Äôt in their training data. That can produce **confident but wrong** answers.

**RAG = Retrieval‚ÄëAugmented Generation.**  
Instead of asking the AI to ‚Äúremember everything,‚Äù we **retrieve the right passages** from **your** data (docs, wikis, spreadsheets, emails, PDFs) at question time and give those passages to the model to ground its response.

Think of it like an open‚Äëbook exam:  
- **Without RAG:** Student answers from memory only.  
- **With RAG:** Student quickly looks up the relevant page in the book and then answers.

**Results:** more accurate, up‚Äëto‚Äëdate, and explainable answers‚Äîwithout retraining the model.

---

## 2) Core Concepts (Plain Language)

### 2.1 Embeddings (turn text into numbers)
We turn text like *‚Äúrefund policy‚Äù* into a list of numbers called a **vector**. This ‚Äúvector‚Äù captures the **meaning** of the text. Similar meanings ‚Üí similar vectors.

```
"refund policy"  ‚Üí  [0.12, -0.45, 0.88, ...]   (vector)
"return policy"  ‚Üí  [0.11, -0.43, 0.90, ...]   (very close in meaning)
```

### 2.2 Vector Databases
A **vector database** stores lots of these vectors (from your documents). When a user asks a question, we:
1) make an **embedding** for the question,  
2) **search** the database for the **most similar** vectors (top‚Äëk results),  
3) feed those text passages to the AI to answer accurately.

Popular vector DBs (any is fine to learn): **Pinecone, Weaviate, Milvus, Qdrant**.

### 2.3 Similarity Search
We measure closeness by **cosine similarity** or **Euclidean distance**. You don‚Äôt need the math; just know ‚Äúhigher similarity = more relevant passage.‚Äù

### 2.4 Chunking
We split your documents into **small pieces** (‚Äúchunks‚Äù) before embedding. Typical chunk sizes: **500‚Äì1,000 tokens** (words-ish), with a small **overlap** so ideas don‚Äôt get cut in half. Good chunking is the #1 quality booster for RAG.

---

## 3) Architecture ‚Äî How RAG Works (Step‚Äëby‚ÄëStep)

### 3.1 High‚ÄëLevel Flow
```
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ     User Ask     ‚îÇ  e.g., ‚ÄúWhat is our refund policy?‚Äù
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Embed the Query ‚îÇ  (text ‚Üí vector)
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Vector DB Search ‚îÇ  (find top‚Äëk similar chunks)
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ   Build Prompt   ‚îÇ  (question + retrieved chunks)
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ    LLM Answer    ‚îÇ  (grounded in your data)
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 What You Need
- **Embedding Model** (e.g., OpenAI embeddings or other vendor)
- **Vector Database** (hosted or self‚Äëhosted)
- **LLM** (OpenAI/Anthropic/etc., or local model)
- **Orchestrator** (we‚Äôll use **n8n** to glue it all together)

---

## 4) Choosing a Vector Database (Simple Guide)

| Option   | Simple to Start | Scales Well | Notes |
|---------|------------------|-------------|-------|
| Pinecone| ‚≠ê‚≠ê‚≠ê‚≠ê            | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | Fully‚Äëmanaged, easy APIs |
| Weaviate| ‚≠ê‚≠ê‚≠ê             | ‚≠ê‚≠ê‚≠ê‚≠ê       | Open‚Äësource & hosted options |
| Milvus  | ‚≠ê‚≠ê              | ‚≠ê‚≠ê‚≠ê‚≠ê       | Popular, performance‚Äëfocused |
| Qdrant  | ‚≠ê‚≠ê‚≠ê             | ‚≠ê‚≠ê‚≠ê‚≠ê       | Open‚Äësource & hosted options |

**Tip:** If you want ‚Äújust works,‚Äù start with **Pinecone** (hosted). If you want open‚Äësource, **Weaviate** or **Qdrant** are great.

---

## 5) Implementing RAG in **n8n** (Copy‚ÄëPastable Steps)

> You can build this *without coding* using n8n‚Äôs nodes. We‚Äôll use generic **HTTP Request** nodes so you can plug in **any** provider. Replace `YOUR_*` placeholders with your real keys/URLs.

### 5.1 Workflow Overview (n8n)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User Trigger ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ Embedding API Call‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ Vector DB Search‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                      ‚îÇ
                                                      ‚ñº
                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                            ‚îÇ  Build RAG Prompt  ‚îÇ
                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                      ‚îÇ
                                                      ‚ñº
                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                            ‚îÇ     LLM Answer     ‚îÇ
                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                      ‚îÇ
                                                      ‚ñº
                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                            ‚îÇ Send Back to User  ‚îÇ
                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.2 Nodes (Step‚Äëby‚ÄëStep)

#### (A) Trigger
- Use **Telegram Trigger**, **Slack Trigger**, or **Webhook**.  
- Output should contain the user‚Äôs **question** in a field like `$json.text` or `$json.query`.

#### (B) Create Embedding (OpenAI example via HTTP Request)
- **Method:** `POST`  
- **URL:** `https://api.openai.com/v1/embeddings`  
- **Headers:**  
  - `Authorization: Bearer {{ $env.OPENAI_API_KEY }}`  
  - `Content-Type: application/json`
- **Body (JSON):**
```json
{
  "input": "{{ $json.query || $json.text }}",
  "model": "text-embedding-3-small"
}
```
- **Mapping tip:** Set a new field `embedding` to `{{$json.data[0].embedding}}` using a **Set** node after the call.

#### (C) Query Vector DB (Pinecone example)
- **Method:** `POST`  
- **URL:** `https://YOUR_INDEX_NAME-YOUR_PROJECT.svc.YOUR_REGION.pinecone.io/query`  
- **Headers:**  
  - `Api-Key: {{ $env.PINECONE_API_KEY }}`  
  - `Content-Type: application/json`
- **Body (JSON):**
```json
{
  "vector": {{ $json.embedding }},
  "topK": 4,
  "includeMetadata": true
}
```
- **Result mapping:** Expect something like `matches[].metadata.text` (varies by your upsert format).

#### (D) Build the RAG Prompt (Set Node)
- **Purpose:** Combine the user question + retrieved passages.  
- **Template:**
```
You are a helpful assistant for our organization.
Answer the question using ONLY the context below. If the answer is not in the context, say ‚ÄúI don‚Äôt have that information.‚Äù

[Context]
{{ $json.matches.map(m => m.metadata.text).join("\n\n---\n\n") }}

[Question]
{{ $json.query || $json.text }}
```
- Store this whole string into a field called `rag_prompt`.

#### (E) Ask the LLM (OpenAI Chat Completions via HTTP)
- **Method:** `POST`  
- **URL:** `https://api.openai.com/v1/chat/completions`  
- **Headers:**  
  - `Authorization: Bearer {{ $env.OPENAI_API_KEY }}`  
  - `Content-Type: application/json`
- **Body (JSON):**
```json
{
  "model": "gpt-4o-mini",
  "messages": [
    { "role": "system", "content": "You only answer from the provided context. If unsure, say you don‚Äôt know." },
    { "role": "user", "content": "{{ $json.rag_prompt }}" }
  ],
  "temperature": 0.2
}
```
- **Pick Output:** `choices[0].message.content` as `final_answer`.

#### (F) Return to User
- Use **Telegram**, **Slack**, or **Webhook Response** to send `$json.final_answer`.

---

## 6) Copy‚ÄëPaste: Example `curl` & Payloads

> Useful for quick tests outside n8n (e.g., in your terminal).

### 6.1 Create an Embedding (OpenAI)
```bash
curl https://api.openai.com/v1/embeddings \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": "What is our refund policy?",
    "model": "text-embedding-3-small"
  }'
```

### 6.2 Pinecone Query (replace URL with your index endpoint)
```bash
curl https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_REGION.pinecone.io/query \
  -H "Api-Key: $PINECONE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "vector": [/* your vector numbers here */],
    "topK": 4,
    "includeMetadata": true
  }'
```

### 6.3 Chat Completion (OpenAI)
```bash
curl https://api.openai.com/v1/chat/completions \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "system", "content": "Answer only from the provided context."},
      {"role": "user", "content": "CONTEXT: ... \n\nQUESTION: What is our refund policy?"}
    ],
    "temperature": 0.2
  }'
```

---

## 7) Preparing Your Data (So Search Actually Works)

**1) Collect Sources:** PDFs, web pages, Google Docs/Sheets, Confluence, emails.  
**2) Clean & Normalize:** Remove headers/footers, fix weird line breaks, convert to plain text.  
**3) Chunk:** 500‚Äì1,000 tokens with **overlap (50‚Äì200)** so ideas aren‚Äôt split.  
**4) Metadata:** Store helpful tags: `title`, `source`, `date`, `url`, `category`, `confidentiality`.  
**5) Upsert:** Embed each chunk and store in your vector DB with metadata.

**Why chunking matters:**  
- Too small ‚áí not enough context.  
- Too big ‚áí more noise and slower search.  
- Sweet spot ‚áí test! Start at ~800 tokens + 100 overlap.

---

## 8) Quality: How to Tell if RAG ‚ÄúWorks‚Äù

- **Precision@k:** When you fetch top‚Äëk passages, are they actually relevant?  
- **Answer faithfulness:** Does the AI answer only using the provided context?  
- **Grounded citations:** Optionally, include which chunk IDs or URLs were used.  
- **Human spot checks:** Create 10‚Äì20 common questions and verify answers.

**Tip:** Lower `temperature` (0.0‚Äì0.3) for more deterministic, factual answers.

---

## 9) Troubleshooting

- **Bad or generic answers?**  
  - Check chunking & metadata.  
  - Increase `topK` (e.g., 3 ‚Üí 6).  
  - Make the system prompt stricter (‚ÄúIf not in context, say you don‚Äôt know‚Äù).

- **No results from vector DB?**  
  - Ensure you actually upserted vectors.  
  - Confirm index name & environment.  
  - Check that you‚Äôre passing the **query embedding** (not raw text).

- **Hallucinations (making stuff up)**  
  - Add: ‚ÄúIf uncertain or not in context, say you don‚Äôt know.‚Äù  
  - Reduce temperature.  
  - Consider smaller context windows with more targeted passages.

- **Costs getting high?**  
  - Use cheaper embedding models.  
  - Cache embeddings (don‚Äôt recompute the same docs).  
  - Pre‚Äëfilter by metadata (e.g., only ‚ÄúRefundPolicy‚Äù docs) before vector search.

---

## 10) Security & Governance

- **Access control:** Filter by user role or department (metadata + pre‚Äëfilters).  
- **PII/Secrets:** Don‚Äôt index secrets; redact sensitive data before upsert.  
- **Data residency:** Choose DB region that satisfies compliance.  
- **Auditability:** Log which chunks were retrieved for each answer.

---

## 11) Best Practices (Quick Wins)

- Good chunking + overlaps  
- Clean text (no page numbers/boilerplate)  
- Add metadata (title, section, URL)  
- Prompt the model to **only** use provided context  
- Keep temperature low for factual tasks  
- Start small, measure, then scale


---

## 12) Frequently Asked Questions (FAQ)

**Q: Do I need to fine‚Äëtune the model?**  
A: Usually no. RAG avoids training costs by retrieving your data at answer time.

**Q: How big can my documents be?**  
A: Any size, as long as you **chunk** them before embedding.

**Q: What if my data is private?**  
A: Host your own vector DB or choose a provider with strong security & region controls. Never index secrets.

**Q: Can I use spreadsheets as sources?**  
A: Yes‚Äîexport to CSV or pull with an API, then chunk rows or sections as text.

**Q: What is ‚ÄútopK‚Äù?**  
A: How many chunks you retrieve. Try 3‚Äì8 and evaluate answer quality.

---

## 13) Sane Defaults (Copy These)

- Chunk size: **800 tokens**, overlap **100**  
- topK: **4**  
- Temperature: **0.2**  
- Prompt guardrails: ‚ÄúAnswer only from context; otherwise say you don‚Äôt know.‚Äù

---

## 14) Glossary (Very Short)

- **Embedding:** Turning text into a vector (numbers) that capture meaning.  
- **Vector:** List of numbers representing text meaning.  
- **Vector DB:** Database optimized to search by vector similarity.  
- **Chunking:** Splitting documents into small pieces before embedding.  
- **topK:** Number of most similar chunks to retrieve.  
- **RAG:** Retrieval‚ÄëAugmented Generation‚ÄîLLM answers are grounded by retrieved text.

---

## 15) Credits & Next Steps

- Explore Pinecone, Weaviate, Milvus, or Qdrant docs.  
- Try different embedding models and chunk sizes.  
- Add role‚Äëbased filtering to respect data access.  
- Measure quality with a small set of real questions.

Happy building! üöÄ

---

[‚¨Ö Back to Course Overview](../../README.md)