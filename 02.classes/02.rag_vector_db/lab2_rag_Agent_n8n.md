# ü§ñ Creaci√≥n de un Agente con RAG en n8n ‚Äî Gu√≠a Paso a Paso

Este documento explica **c√≥mo crear un agente con RAG (Retrieval-Augmented Generation)** en n8n, basado en el diagrama de flujo mostrado en la imagen de referencia. Incluye la funci√≥n de cada nodo, c√≥mo configurarlo y c√≥mo se conecta en el flujo.


## üìå Objetivo
Construir un agente que:
1. Reciba mensajes de un chat.
2. Pueda razonar y decidir cu√°ndo buscar informaci√≥n en una base vectorial (Pinecone).
3. Genere respuestas basadas en datos reales usando RAG.

---

## üõ† Estructura del flujo

### **1. Nodo de disparo (Trigger)**
- **Nombre:** `When chat message received`
- **Funci√≥n:** Captura los mensajes que env√≠a el usuario desde un canal como Telegram, Slack o un chat web.
- **Salida:** Texto del usuario (`query`), que ser√° la base para todo el proceso.

---

### **2. Nodo `AI Agent` (Tools Agent)**
- **Funci√≥n:** Nodo principal que orquesta la conversaci√≥n.
- **Recibe:**
  - Modelo de chat para procesar y razonar (`OpenAI Chat Model`).
  - Memoria de la conversaci√≥n (`Window Buffer Memory`).
  - Herramientas adicionales como la b√∫squeda en un vector store.
- **Configuraci√≥n:**
  - Tipo de agente: **Tools Agent**.
  - Instrucciones del rol: Indicar que debe usar el vector store para buscar informaci√≥n relevante antes de responder.
- **Conexiones:**
  - Entrada: desde el **Trigger**.
  - Salidas: hacia nodos de modelo, memoria y herramientas.

---

### **3. Nodo `OpenAI Chat Model` (vinculado al AI Agent)**
- **Funci√≥n:** Define el modelo LLM principal que usar√° el agente para interpretar y razonar.
- **Ejemplo de configuraci√≥n:**
  - Modelo: `gpt-4o-mini` o `gpt-4`.
  - Temperatura: baja (0.2‚Äì0.3) para respuestas precisas.
- **Conexi√≥n:** Desde el `AI Agent` (como **Chat Model**).

---

### **4. Nodo `Window Buffer Memory`**
- **Funci√≥n:** Mantiene un historial limitado de mensajes para dar contexto.
- **Ejemplo:**
  - Guardar las √∫ltimas 5‚Äì10 interacciones.
- **Conexi√≥n:** Desde el `AI Agent` (como **Memory**).

---

### **5. Nodo `Answer questions with a vector store`**
- **Funci√≥n:** Herramienta que conecta el agente con la base de conocimiento v√≠a RAG.
- **Acci√≥n:** Recibe la consulta del usuario y busca informaci√≥n relevante en la base vectorial antes de responder.
- **Conexiones:**
  - **VectorStoreModel:** conectado a un modelo de chat (`OpenAI Chat Model1`).
  - **Vector Store:** conectado a `Pinecone Vector Store`.

---

### **6. Nodo `Pinecone Vector Store`**
- **Funci√≥n:** Base de datos vectorial que almacena los embeddings de tus documentos.
- **Flujo:**
  1. Recibe la consulta en forma de vector.
  2. Busca coincidencias m√°s cercanas (top-k).
  3. Devuelve fragmentos relevantes como contexto.
- **Conexi√≥n:**
  - Entrada **Embedding**: desde `Embeddings OpenAI`.
  - Salida **Vector Store**: hacia `Answer questions with a vector store`.

---

### **7. Nodo `Embeddings OpenAI`**
- **Funci√≥n:** Convierte el texto en un vector num√©rico (embedding) para buscar en Pinecone.
- **Modelo recomendado:** `text-embedding-3-small` (r√°pido y econ√≥mico) o `text-embedding-3-large` (m√°s preciso).
- **Conexi√≥n:**
  - Entrada: texto de la consulta (pasado por la herramienta).
  - Salida: vector hacia `Pinecone Vector Store`.

---

### **8. Nodo `OpenAI Chat Model1` (VectorStoreModel)**
- **Funci√≥n:** LLM que se usa dentro de la herramienta RAG para generar la respuesta usando el contexto recuperado.
- **Diferencia con el primer modelo de chat:**
  - El primer `OpenAI Chat Model` es para el agente general.
  - Este segundo modelo se usa exclusivamente cuando el agente decide consultar el vector store.

---

## üîÑ Flujo de ejecuci√≥n completo
1. Usuario env√≠a un mensaje ‚Üí `When chat message received`.
2. El mensaje llega al **AI Agent**.
3. El agente:
   - Usa su modelo principal (`OpenAI Chat Model`) para razonar.
   - Consulta la memoria (`Window Buffer Memory`) para mantener el contexto.
   - Decide usar la herramienta RAG (`Answer questions with a vector store`).
4. La herramienta RAG:
   1. Convierte la consulta en embedding (`Embeddings OpenAI`).
   2. Busca en la base vectorial (`Pinecone Vector Store`).
   3. Pasa los fragmentos relevantes a `OpenAI Chat Model1`.
5. El modelo genera una respuesta usando solo la informaci√≥n recuperada.
6. El agente env√≠a la respuesta al usuario.

El flujo deber√≠a lucir as√≠:

![](../../04.assets/images/rag_agent.png)


---

## üí° Consejos
- Ajusta `topK` en Pinecone para mejorar la cantidad de contexto (3‚Äì8 recomendado).
- Usa `temperature` baja para evitar respuestas inventadas.
- Mant√©n los datos de Pinecone actualizados para asegurar respuestas precisas.

---

## üìé Recursos
- [n8n Documentation](https://docs.n8n.io)
- [Pinecone Documentation](https://docs.pinecone.io)
- [OpenAI API Docs](https://platform.openai.com/docs)

---

[‚¨Ö Back to Course Overview](../../README.md)
